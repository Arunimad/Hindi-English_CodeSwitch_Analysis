{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d25fe045-80d5-46f8-ae2e-215d5cacd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import random\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "71594c6a-8e4e-4dcb-92fb-3f49177f652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_English.csv\", encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv(\"data/test_English.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f6bc025-5e42-45dd-aae9-55655cb7cd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                         OriginalTweet  \\\n",
       "0  cb774db0d1   I`d have responded, if I were going   \n",
       "\n",
       "                         selected_text Sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['OriginalTweet'][0:1]\n",
    "df_train[0:0]\n",
    "df_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2527800f-ed81-49b1-986d-8941d6de7b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   textID            27481 non-null  object \n",
      " 1   OriginalTweet     27480 non-null  object \n",
      " 2   selected_text     27480 non-null  object \n",
      " 3   Sentiment         27481 non-null  object \n",
      " 4   Time of Tweet     27481 non-null  object \n",
      " 5   Age of User       27481 non-null  object \n",
      " 6   Country           27481 non-null  object \n",
      " 7   Population -2020  27481 non-null  int64  \n",
      " 8   Land Area (Km²)   27481 non-null  float64\n",
      " 9   Density (P/Km²)   27481 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n",
    "#location a onek non-null value\n",
    "#baki kothao non-null nai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a916c3e2-60a6-47bc-8bbf-aaa7d8ae38e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   textID            27481 non-null  object \n",
      " 1   OriginalTweet     27480 non-null  object \n",
      " 2   selected_text     27480 non-null  object \n",
      " 3   Sentiment         27481 non-null  object \n",
      " 4   Time of Tweet     27481 non-null  object \n",
      " 5   Age of User       27481 non-null  object \n",
      " 6   Country           27481 non-null  object \n",
      " 7   Population -2020  27481 non-null  int64  \n",
      " 8   Land Area (Km²)   27481 non-null  float64\n",
      " 9   Density (P/Km²)   27481 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#drop duplicate tweet\n",
    "df_train.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da3989b2-299f-40fe-b4d1-3ce7a1393608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_temp = df_train[['OriginalTweet','Sentiment']]\n",
    "df_train.head()['OriginalTweet'].values\n",
    "df_train = df_train.copy()\n",
    "df_test_temp = df_train[['OriginalTweet','Sentiment']]\n",
    "df_test.head()['OriginalTweet'].values\n",
    "df_test = df_test.copy()\n",
    "\n",
    "#@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
    "#mention gula meaning less\n",
    "#telegram link ero kono kaj nai \n",
    "#eigula remove kora lagbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d21295ac-4537-46cc-8101-11b3e910a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#print(device)\n",
    "#if device.type == 'cuda':\n",
    "#    print(torch.cuda.get_device_name(0))\n",
    "#    print('Memory Usage:')\n",
    "#    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "#else:\n",
    "#    exit(0)\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8385eaee-9564-4848-b2ae-e505810cd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(row):\n",
    "    # https://radimrehurek.com/gensim/parsing/preprocessing.html\n",
    "    #mainly to remove whitespaces\n",
    "    #remove all kinds of links from data\n",
    "    # also remove mentions like @\n",
    "    #first row should be and and\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', row)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\S+', ' ', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#', ' ', text)\n",
    "    text = text.replace('.', ' ').replace(',', ' ')\n",
    "    # Remove newline and carriage return characters\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a2523928-8921-45d0-b9e1-7ab54e4cc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with empty strings\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].fillna('')\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].fillna('')\n",
    "\n",
    "# Ensure all entries are strings\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].astype(str)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "710504db-54dd-4483-8a38-b1d0d7d8c058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 Last session of the day\n",
       "1       Shanghai is also really exciting precisely sky...\n",
       "2       Recession hit Veronique Branquinho she has to ...\n",
       "3                                              happy bday\n",
       "4                                               I like it\n",
       "                              ...                        \n",
       "3529    its at 3 am im very tired but i cant sleep but...\n",
       "3530    All alone in this old house again Thanks for t...\n",
       "3531    I know what you mean My little dog is sinking ...\n",
       "3532    _sutra what is your next youtube video gonna b...\n",
       "3533                               omgssh ang cute ng bby\n",
       "Name: CleanTweet, Length: 3534, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['CleanTweet']=df_train['OriginalTweet'].apply(preprocess)\n",
    "df_train.head()['CleanTweet'].values\n",
    "df_train = df_train[['CleanTweet','Sentiment']]\n",
    "df_train['CleanTweet']\n",
    "\n",
    "df_test['CleanTweet']=df_test['OriginalTweet'].apply(preprocess)\n",
    "df_test.head()['CleanTweet'].values\n",
    "df_test = df_test[['CleanTweet','Sentiment']]\n",
    "df_test['CleanTweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6cd95f04-a3ff-47b4-9186-5f7eabede5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sooo SAD I will miss you here in San Diego',\n",
       "       'my boss is bullying me', 'what interview leave me alone',\n",
       "       'Sons of why couldnt they put them on the releases we already bought',\n",
       "       'some shameless plugging for the best Rangers forum on earth',\n",
       "       '2am feedings for the baby are fun when he is all smiles and coos',\n",
       "       'Soooo high'], dtype=object)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['CleanTweet'][1:8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "55fcc07b-4158-4c8d-920f-011d6230ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   CleanTweet  27481 non-null  object\n",
      " 1   Sentiment   27481 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 429.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train=df_train[df_train['CleanTweet'] != ' ']\n",
    "df_test=df_test[df_test['CleanTweet'] != ' ']\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3d5e3169-4512-421a-8f25-dcb1084d15fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "Sentiment\n",
      "1    11118\n",
      "2     8582\n",
      "0     7781\n",
      "Name: count, dtype: int64\n",
      "Sentiment\n",
      "neutral     1430\n",
      "positive    1103\n",
      "negative    1001\n",
      "Name: count, dtype: int64\n",
      "Sentiment\n",
      "1    1430\n",
      "2    1103\n",
      "0    1001\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#now we need to work with Sentiment values...they are 5 types \n",
    "print(df_train['Sentiment'].value_counts())\n",
    "df_train['Sentiment'] = df_train['Sentiment'].map({'negative':0,'neutral':1,'positive':2})\n",
    "print(df_train['Sentiment'].value_counts())\n",
    "\n",
    "print(df_test['Sentiment'].value_counts())\n",
    "df_test['Sentiment'] = df_test['Sentiment'].map({'negative':0,'neutral':1,'positive':2})\n",
    "print(df_test['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1d621070-bc90-47a2-939c-6dbab22bec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_train['sentiment'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a7a732b1-32ca-4198-99aa-3e2b4fe00f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df_train.dropna(subset=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9a546a58-e5f8-46d5-b369-57d0dd115e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14668 20766 23037 ... 11017 25564 18322]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_type\n",
       "train    23358\n",
       "val       4123\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_train.index.values, \n",
    "                                                  df_train.Sentiment.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df_train.Sentiment.values)\n",
    "print(X_train)\n",
    "df_train['data_type'] = ['not_set']*df_train.shape[0]\n",
    "#mark training and validation data\n",
    "df_train.loc[X_train, 'data_type'] = 'train'\n",
    "df_train.loc[X_val, 'data_type'] = 'val'\n",
    "df_train[:]\n",
    "df_train['data_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cba557b6-39d5-49da-af05-65f2bd026d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating BertTokenizer from 'bert-base-uncased' model.\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer \n",
    "model_name = \"google/muril-base-cased\" \n",
    "from transformers import BertTokenizer, ElectraForSequenceClassification, RobertaForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9260230f-e501-4f2d-8797-758dbd8421ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Id have responded if I were going',\n",
       "       'Sooo SAD I will miss you here in San Diego',\n",
       "       'my boss is bullying me', ...,\n",
       "       'Yay good for both of you Enjoy the break you probably need it after such hectic weekend Take care hun xxxx',\n",
       "       'But it was worth it',\n",
       "       'All this flirting going on The ATG smiles Yay hugs'], dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.data_type=='train'].CleanTweet.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8fb6575c-beb7-487a-a443-620d1cb661cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train[df_train.data_type=='train'].CleanTweet.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='longest',\n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "89684e4a-1ec0-4525-98ca-7f7fb35cbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    df_test.CleanTweet.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='longest',\n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "de5acf03-bb1a-4052-a5b3-d9a4285d83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_train = encoded_data_train['input_ids']\n",
    "# This line extracts the input_ids from the encoded_data_train dictionary and assigns them to the variable input_ids_train.\n",
    "# encoded_data_train likely contains the tokenized input data, where input_ids are numerical representations of the tokens.\n",
    "\n",
    "\n",
    "# attention_masks_train = encoded_data_train['attention_mask']\n",
    "# This line extracts the attention_mask from the encoded_data_train dictionary and assigns them to the variable attention_masks_train.\n",
    "# attention_masks are used in models like BERT to differentiate between actual tokens and padding tokens, ensuring that the model only attends to the real tokens.\n",
    "\n",
    "\n",
    "\n",
    "# labels_train = torch.tensor(df_train[df_train.data_type=='train'].Sentiment.values)\n",
    "# This line creates a tensor of training labels using the torch.tensor function from the PyTorch library.\n",
    "# df_train is likely a DataFrame containing the training data.\n",
    "# df_train[df_train.data_type=='train'] filters the DataFrame to include only the rows where the data_type column is equal to 'train'.\n",
    "# .Sentiment.values extracts the sentiment labels from the filtered DataFrame.\n",
    "# The resulting labels are converted into a PyTorch tensor, which can be used in the training process.\n",
    "\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train[df_train.data_type=='train'].Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ad974016-bf00-4ee6-a618-2c430988fcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  8909,  2031,  ...,     0,     0,     0],\n",
      "        [  101, 17111,  2080,  ...,     0,     0,     0],\n",
      "        [  101,  2026,  5795,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  8038,  2100,  ...,     0,     0,     0],\n",
      "        [  101,  2021,  2009,  ...,     0,     0,     0],\n",
      "        [  101,  2035,  2023,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([1, 0, 0,  ..., 2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_train)\n",
    "print(attention_masks_train)\n",
    "print(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "17e4bde5-5a1a-4854-95ad-3cbf26f04121",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df_train[df_train.data_type=='val'].CleanTweet.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='longest',\n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df_train[df_train.data_type=='val'].Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "448da444-6ba9-4345-9518-d1aa203fa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(df_test.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6c37be48-e5fc-4c7a-b241-3a989e20a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23358 4123\n",
      "3534\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "print(len(dataset_train), len(dataset_val))\n",
    "print(len(dataset_test))\n",
    "#training set ar test set er encoding thik ase // total count o ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "21ff0bc7-6d41-44d5-bb09-784871de5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=3,\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "# num_labels==  specifies the number of output labels the model should predict.\n",
    "# output_attentions=False and output_hidden_states=False are optional parameters that,\n",
    "# when set to False, prevent the model from returning attention weights and hidden states, respectively, which can save memory during training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "85ee1f1e-228d-4716-ab6d-4685732f9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)\n",
    "dataloader_test = DataLoader(dataset_test,\n",
    "                             sampler=SequentialSampler(dataset_test),\n",
    "                             batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "652e8c1d-87e1-4075-9a9a-adecf5741f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "978c0262-641e-42eb-8b01-cd1d5f5dd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we fix epoch number and scheduler\n",
    "epochs = 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f5877195-01ba-4312-809f-b7642e5e088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, file_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f5791075-a1e5-4497-a2ae-4a1ba9129f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee46d759295343308f5ffc7b255baeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    # Training code...\n",
    "\n",
    "    # Save the checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, scheduler, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7bfc6379-f271-492e-9d9c-ad125699584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(file_path, model, optimizer, scheduler):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cefd0211-961b-4be8-a958-bdc756eb47bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb240684cd7b4a09b4363626ab5dd35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have the model, optimizer, and scheduler already defined\n",
    "start_epoch = 0\n",
    "checkpoint_path = 'checkpoint.pth'\n",
    "\n",
    "try:\n",
    "    start_epoch = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
    "    print(f\"Resuming from epoch {start_epoch + 1}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found, starting from scratch.\")\n",
    "\n",
    "for epoch in tqdm(range(start_epoch + 1, epochs + 1)):\n",
    "    model.train()\n",
    "\n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2],\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': f'{loss.item() / len(batch):.3f}'})\n",
    "\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "\n",
    "    # Save the checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, scheduler, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ee70f955-3a52-4247-a2ee-286902632867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "83b33b63-58e1-476e-a6b4-fd9a368492b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3b7177bf-867a-4abc-bb55-6cd9b1cb0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ead1734b-4049-4ede-b811-6967098a2e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ea158c5a-574d-4021-bd15-4a7108806d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26371cee106d48f7946ce98405f46699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.9285103312986966\n",
      "Validation loss: 0.8339783930292879\n",
      "F1 Score (Weighted): 0.6225144668162947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.7546149021601432\n",
      "Validation loss: 0.84985072955229\n",
      "F1 Score (Weighted): 0.7066631227351663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.714198288313362\n",
      "Validation loss: 0.8599191431848605\n",
      "F1 Score (Weighted): 0.7066245479785479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.7029889863906174\n",
      "Validation loss: 0.8808040382253832\n",
      "F1 Score (Weighted): 0.7172319698946047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.6722044555267817\n",
      "Validation loss: 0.9656074087846407\n",
      "F1 Score (Weighted): 0.7171147107339907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Training loss: 0.6773417292139253\n",
      "Validation loss: 1.0834482037173232\n",
      "F1 Score (Weighted): 0.7263918363244478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Training loss: 0.6388371267780576\n",
      "Validation loss: 1.2873940555476804\n",
      "F1 Score (Weighted): 0.7218619039928906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Training loss: 0.6187493674791169\n",
      "Validation loss: 1.2705538887729295\n",
      "F1 Score (Weighted): 0.7225381033258682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Training loss: 0.5780780265859068\n",
      "Validation loss: 1.3857478378706447\n",
      "F1 Score (Weighted): 0.7150732165131427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "Training loss: 0.5561710679701694\n",
      "Validation loss: 1.378443002393564\n",
      "F1 Score (Weighted): 0.7175830190775107\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "                 \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ed6e1011-8cbd-470c-8e31-7e312da2e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_generation(preds, labels):\n",
    "    label_dict = {'negative':0,'neutral':1,'positive':2}\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "        P_c = precision_score(labels_flat, preds_flat, average=None, labels=[label])[0]\n",
    "        R_c = recall_score(labels_flat, preds_flat, average=None, labels=[label])[0]\n",
    "        F1_c = f1_score(labels_flat, preds_flat, average=None, labels=[label])[0]\n",
    "\n",
    "        print(f\"=*= {label_dict_inverse[label]} =*=\")\n",
    "        # print(\"Full precision:\\t\",P_c)\n",
    "        # print(\"Full recall:\\t\\t\",R_c)\n",
    "        # print(\"Full F1 score:\\t\",F1_c)\n",
    "        print(f\"precision:\\t{P_c:.4f}\")\n",
    "        print(f\"recall:\\t\\t{R_c:.4f}\")\n",
    "        print(f\"F1 score:\\t{F1_c:.4f}\")\n",
    "        print()\n",
    "\n",
    "    P = precision_score(labels_flat, preds_flat, average='micro')\n",
    "    R = recall_score(labels_flat, preds_flat, average='micro')\n",
    "    F1 = f1_score(labels_flat, preds_flat, average='micro')\n",
    "\n",
    "    print(\"=*= global =*=\")7\n",
    "    print(f\"precision:\\t{P:.4f}\")\n",
    "    print(f\"recall:\\t\\t{R:.4f}\")\n",
    "    print(f\"F1 score:\\t{F1:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9a47e90f-515d-4717-8245-407cb0c3e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateTest(model, dataloader_val):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2aee40be-4d7e-4883-b612-8a2eea569204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "17d88e94-dbaa-4672-b88a-36ced3d56ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: negative\n",
      "Accuracy: 722/1001\n",
      "\n",
      "=*= negative =*=\n",
      "precision:\t0.7010\n",
      "recall:\t\t0.7213\n",
      "F1 score:\t0.7110\n",
      "\n",
      "Class: neutral\n",
      "Accuracy: 998/1430\n",
      "\n",
      "=*= neutral =*=\n",
      "precision:\t0.6911\n",
      "recall:\t\t0.6979\n",
      "F1 score:\t0.6945\n",
      "\n",
      "Class: positive\n",
      "Accuracy: 850/1103\n",
      "\n",
      "=*= positive =*=\n",
      "precision:\t0.8019\n",
      "recall:\t\t0.7706\n",
      "F1 score:\t0.7859\n",
      "\n",
      "=*= global =*=\n",
      "precision:\t0.7272\n",
      "recall:\t\t0.7272\n",
      "F1 score:\t0.7272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, predictions, true_vals = evaluateTest(model, dataloader_test)\n",
    "result_generation(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67689bfb-0980-419d-bcd9-3a23d630add2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be47a4-b624-45b1-a527-2beb58930a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
